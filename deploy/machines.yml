
# This file is part of HemeLB and is Copyright (C)
# the HemeLB team and/or their institutions, as detailed in the
# file AUTHORS. This software is provided under the terms of the
# license in the file LICENSE.

default: 
  # General default values for all machines live in the default dictionary.
  # Machine-specific overrides replace these.
  # git server to find HemeLB
  github: "git@github.com"
  # Github user (configurable for forks)
  github_user: "UCL-CCS"
  # Repository containing HemeLB
  hemelb_repo: "hemelb-dev"
  # Repository holding the RegressionTests
  regression_tests_repo: "hemelb-tests"
  # Default Modules to load or unload on remote, via module 'foo'
  # e.g. ['load cmake'].
  modules: []
  # Commands to execute before attempting a remote build.
  build_prefix_commands: []
  # Commands to execute as part of remote run jobs.
  run_prefix_commands: []
  # Templates to use to find space to checkout and build HemeLB on the remote.
  # All user- and generally- defined values in these config dictionaries will be interpolated via $foo
  home_path_template: "/home/$username"
  # Name for the Project Fabric folder
  fabric_dir: "FabricHemeLb"
  # Path to runtime accessible filesystem (default is same as build time)
  runtime_path_template: "$home_path"
  make_jobs: 1
  corespernode: 1
  # Default options used in the CMake configure step.
  # Command to use to launch remote jobs
  # e.g. qsub, empty string to launch interactively.
  job_dispatch: ''
  # Path to build filesystem folder
  # All user-defined values in these config dictionaries will be interpolated via $foo
  remote_path_template: "$home_path/$fabric_dir"
  # Path to run filesystem folder
  work_path_template: "$runtime_path/$fabric_dir"
  install_path_template: "$remote_path/install"
  # Normally, the regression test can be run directly out of diffTest, but sometimes needs to be copied to a runtime file area.
  regression_test_path_template: "$regression_test_source_path"
  python_build: "TODO"
  batch_header: no_batch
  run_command: "mpirun -np $cores"
  temp_path_template: "/tmp"
  cmake_options: {}
  job_name_template: '${config}_${machine_name}_${build_number}_${submit_time}'
  config_name_template: "${profile}_${StringVoxelSize}_${Steps}_${Cycles}"
  local_templates_path: "$localroot/deploy/templates"
  manual_ssh: false
  local_templates_path: "$localroot/deploy/templates"
  stat: qstat
  forward_agent: true

archer: #the ARCHER supercomputer at EPCC
  import: "archer"
  max_job_name_chars: 15
  make_jobs: 4
  verbose: 1
  job_dispatch: "qsub"
  run_command: "aprun -n $cores -N $corespernode > stdout.txt 2> stderr.txt"
  batch_header: pbs-archer
  no_ssh: true
  no_git: true
  remote: "login.archer.ac.uk"
  no_git: true # sync from local checkout, easier to deploy local changes for debug, DON'T MERGE IN
  # On ARCHER, *all files* which are needed at runtime, must be on the /work filesystem, so we must make the install location be on the /work filesystem
  install_path_template: "$work_path/install"
  home_path_template: "/home/$project/$project/$username"
  runtime_path_template: "/work/$project/$project/$username"
  fabric_dir: "FabricHemeLb"
  modules: ["load cmake", "swap PrgEnv-cray PrgEnv-gnu", "load boost", "load cray-hdf5-parallel", "load cray-tpsl", "load openssl wget"]
  build_prefix_commands: ["export LDFLAGS=-dynamic"] # Required for HDF5 to compile
  temp_path_template: "$work_path/tmp"
  regression_test_path_template: "$work_path/regression"
  queue: "standard"
  python_build: "lib/python2.7"
  corespernode: 24
  cmake_options:
    HEMELB_OPTIMISATION: "-O3"
    HEMELB_USE_SSE3: "ON"
    HEMELB_COMPUTE_ARCHITECTURE: "INTELSANDYBRIDGE"
    METIS_ROOT: "$CRAY_TPSL_PREFIX_DIR"

legion: 
  job_dispatch: "qsub"
  run_command: "mpirun -np $cores -machinefile $$TMPDIR/machines"
  batch_header: sge
  # The remote host-name to use.
  remote: "legion.rc.ucl.ac.uk"
  # The path python disttools use for builds.
  python_build: "python2.7"
  modules: ["remove mpi/qlogic/1.2.7/intel","remove compilers/intel/11.1/072","add compilers/gnu/4.1.2", "add mpi/openmpi/1.4.1/gnu", "add cmake/2.8.3", "add python/enthought/7.2-2"]
  runtime_path_template: "$home_path/Scratch"
  node_type_restriction_template: '#$$ -ac allow="${node_type}"'
  node_type: "X"
  corespernode: 12
  temp_path_template: false #Legion sets its own tmpdir
legion_vampir:
  fabric_dir: "VampirHemeLB"
  import: legion
  build_prefix_commands: ["export CXX=mpicxx-vt","export CC=mpicc-vt", "export LD=mpicc-vt"] #Tell autoconf for dependencies where the compilers are
  cmake_options: 
    CMAKE_CXX_COMPILER: "mpicxx-vt"
    CMAKE_C_COMPILER: "mpicc-vt"
legion_intel:
  fabric_dir: "IntelHemeLB"
  import: legion
  modules: ["remove mpi/qlogic/1.2.7/intel","add mpi/openmpi/1.4.1/intel","add cmake/2.8.3", "add python/2.6.6/gnu"]
  # Tell autoconf for dependencies where the compilers are
  build_prefix_commands: ["export CXX=icpc","export CC=icc", "export LD=icc"]
  cmake_options: 
    HEMELB_USE_ALL_WARNINGS_GNU: "OFF"
    CMAKE_CXX_COMPILER: "icpc"
    CMAKE_C_COMPILER: "icc"
    CMAKE_CXX_FLAGS_RELEASE: ""
    HEMELB_OPTIMISATION: "-O3"
    HEMELB_DEPENDENCIES_SET_RPATH: "OFF"
planck:
  needs_tarballs: true
  remote: "planck.chem.ucl.ac.uk"
  python_build: "lib/python2.7"
  home_path_template: "/home/$username/.fabric/Remote"
entropy: 
  remote: "entropy.chem.ucl.ac.uk"
  python_build: "lib/python2.6"
  runtime_path_template: "/store4/blood/$username"
supermuc:
  max_job_name_chars: 15
  make_jobs: 4
  job_dispatch: "llsubmit"
  run_command: "mpiexec -n $cores"
  batch_header: ll
  no_ssh: true # doesn't allow outgoing ssh sessions.
  remote: "supermuc.lrz.de"
  # On supermuc, *all files* which are needed at runtime, must be on the /work filesystem, so we must make the install location be on the /work filesystem
  install_path_template: "/home/hpc/pr45su/lu64bap/install"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/home/hpc/$project/$username"
  modules: ["load cmake/2.8","load mpi.ibm"]
  # Tell autoconf for dependencies where the compilers are
  build_prefix_commands: ["export CXX=mpiCC","export CC=mpicc", "export LD=mpiCC"]
  temp_path_template: "$work_path/tmp"
  regression_test_path_template: "$work_path/regression"
  queue: "parallel"
  python_build: "lib64/python2.6"
  corespernode: 16
  cmake_options:
    HEMELB_USE_ALL_WARNINGS_GNU: "OFF"
    CMAKE_CXX_COMPILER: "CC"
    CMAKE_C_COMPILER: "cc"
    CMAKE_CXX_FLAGS_RELEASE: ""
    HEMELB_OPTIMISATION: "-O3"
    HEMELB_DEPENDENCIES_SET_RPATH: "OFF"
    CTEMPLATE_PATCH_ALIGN: "ON"
    MPWide_INCLUDE_DIR: "/home/hpc/pr45su/lu64bap/mpwide"
    MPWide_LIBRARIES: "/home/hpc/pr45su/lu64bap/mpwide/libMPW.a"
hermit:
  max_job_name_chars: 15
  make_jobs: 4
  job_dispatch: "qsub"
  run_command: "aprun -n $cores -N $coresusedpernode"
  batch_header: pbs_barebone
  no_ssh: true # Hermit doesn't allow outgoing ssh sessions.
  remote: "hermit1.hww.de"
  # On hermit, *all files* which are needed at runtime, must be on the /work filesystem, so we must make the install location be on the /work filesystem
  install_path_template: "$work_path/install"
  home_path_template: "/zhome/academic/HLRS/pri/$username"
  runtime_path_template: "/zhome/academic/HLRS/pri/$username"
  modules: ["load tools/cmake/2.8.7"]
  # Tell autoconf for dependencies where the compilers are
  build_prefix_commands: ["export CXX=CC","export CC=cc", "export LD=CC"]
  temp_path_template: "$work_path/tmp"
  regression_test_path_template: "$work_path/regression"
  python_build: "lib64/python2.6"
  corespernode: 32
  cmake_options:
    HEMELB_USE_ALL_WARNINGS_GNU: "OFF"
    CMAKE_CXX_COMPILER: "CC"
    CMAKE_C_COMPILER: "cc"
    CMAKE_CXX_FLAGS_RELEASE: ""
    HEMELB_OPTIMISATION: "-O3"
    HEMELB_DEPENDENCIES_SET_RPATH: "OFF"
    CTEMPLATE_PATCH_ALIGN: "ON"
julian:
   home_path_template: "/Users/$username"
   remote: "julian.chem.ucl.ac.uk"
   python_build: "lib/python2.7"
oppenheimer:
   remote: "oppenheimer.chem.ucl.ac.uk"
   run_command: "/opt/openmpi/gfortran/1.6.5/bin/mpirun -np $cores"
   batch_header: sge_oppenheimer
   build_prefix_commands: ["export set PATH=$PATH:/home/$username/install/bin","export set PYTHONPATH=$PYTHONPATH:/home/$username/install/lib/python2.6/site-packages"]
   no_git: true
   job_dispatch: "qsub"
   python_build: "lib/python2.6"
   make_jobs: 4
localhost:
   remote: "localhost"
   python_build: "lib/python2.7"
   make_jobs: 4
huygens:
    remote: "huygens.sara.nl"
    username: "sar00033"
    no_git: true
    home_path_template: "/home/$username/hemelb"
    modules: ["load cmake/2.8.5","load gcc"]
    build_prefix_commands: []
    temp_path_template: /scratch/shared
    regression_test_path_template: "$work_path/regression"
    corespernode: 32
    job_dispatch:  llsubmit
    stat: llq
    needs_tarballs: true
    python_build: 'lib64/python2.6'
    cmake_options: 
      HEMELB_USE_ALL_WARNINGS_GNU: "OFF"
      CMAKE_CXX_COMPILER: "mpCC"
      CMAKE_C_COMPILER: "mpcc"
      CMAKE_CXX_FLAGS_RELEASE: ""
      HEMELB_OPTIMISATION: "-O3"
      CTEMPLATE_CONFIGURE_OPTIONS: --host=powerpc64-unknown-linux-gnu
   
riken:
   remote: "127.0.0.1:2200"  #Not a place to run code, just to get the via code there to run on Marigold
   no_ssh: true
   no_git: true
   needs_tarballs: true
# Before the marigold section will work, you need to set up your LOCAL ssh config as follows:
# Host riken
#  HostName 127.0.0.1
#  Port 2200
#  User jamespjh
#  Host marigold
#  ProxyCommand ssh -q riken nc -w60 10.1.40.31 22
#  Host marigold_run
#  ProxyCommand ssh -q marigold nc -w60 mg 22
# And also, have followed the RIKEN instructions to join the RIKEN VPN with localhost:2200 forwarding to Riken.
marigold:
   remote: marigold
   no_ssh: true
   no_git: true
   needs_tarballs: true
   manual_ssh: true
   runtime_path_template: "/data/$username"
   install_path_template: "$work_path/install"
   build_prefix_commands:
     - export CXX=/opt/FJSVtclang/1.2.0/bin/mpiFCCpx
     - export CC=/opt/FJSVtclang/1.2.0/bin/mpifccpx
     - export CPPFLAGS="-DNO_THREADS"
     - export CXXFLAGS=""
   cmake_options:
     CMAKE_CXX_COMPILER: /opt/FJSVtclang/1.2.0/bin/mpiFCCpx
     CMAKE_C_COMPILER: /opt/FJSVtclang/1.2.0/bin/mpifccpx
     CTEMPLATE_CONFIGURE_OPTIONS: --host=sparc64-unknown-linux-gnu
     PARMETIS_CXX: /opt/FJSVtclang/1.2.0/bin/mpiFCCpx
     PARMETIS_CC: /opt/FJSVtclang/1.2.0/bin/mpifccpx
     CTEMPLATE_PATCH_VACOPY: ON
     CTEMPLATE_PATCH_ALIGN: ON

marigold_run:
  import: marigold
  remote: marigold_run
